# ğŸ” Workflow Hang Diagnosis

## ğŸ“Š Analisi dels Logs

**Data:** 2025-10-23 16:27-16:29  
**Container:** RunPod amb RTX 4090 (22GB VRAM)  
**Workflow:** WanVideo I2V (Image to Video)

---

## âŒ Problema Identificat

### SÃ­mptomes:

1. âœ… ComfyUI arrenca correctament
2. âœ… Workflow comenÃ§a (`got prompt`)
3. âœ… Model carrega correctament (WanVideo 14B)
4. âŒ **El sampling es para al 0%**
5. âŒ **Container es reinicia automÃ ticament**
6. âŒ **No hi ha error explÃ­cit als logs**

### Punt exacte de fallada:

```
2025-10-23T16:28:02.618719777Z Sampling 121 frames at 704x704 with 3 steps
  0%|          | 0/3 [00:00<?, ?it/s]
2025-10-23T16:28:13.007846741Z
[CONTAINER RESTART - CUDA banner apareix]
```

**Temps transcorregut:** ~10 segons fins restart.

---

## ğŸ¯ Causa MÃ©s Probable: **Out of Memory (OOM)**

### EvidÃ¨ncia:

1. **CÃ rrega de memÃ²ria massiva:**

   - Model: WanVideo I2V 14B (~28GB amb pesos FP8)
   - ResoluciÃ³: 704x704
   - Frames: 121
   - VRAM disponible: 22GB RTX 4090

2. **CÃ lcul aproximat:**

   ```
   Model base: ~14GB
   Activations per frame: ~150MB Ã— 121 = ~18GB
   Overhead PyTorch: ~2-3GB
   TOTAL: ~35GB necessaris
   DISPONIBLE: 22GB
   DÃˆFICIT: ~13GB âŒ
   ```

3. **Pattern tÃ­pic d'OOM:**
   - Arrenca correctament
   - Model carrega OK
   - Crash durant sampling (quan s'alÂ·loquen activations)
   - Restart silenciÃ³s (kernel mata el procÃ©s)

### Per quÃ¨ no hi ha error?

**OOM de CUDA mata el procÃ©s directament** sense donar temps a Python de capturar l'excepciÃ³. RunPod reinicia automÃ ticament el container.

---

## âœ… Solucions

### 1. **Reduir cÃ rrega de memÃ²ria** (Recomanat)

#### A. Al Workflow (UI de ComfyUI):

```
ResoluciÃ³: 704x704 â†’ 512x512 o 480x360
Frames: 121 â†’ 60 o menys
Batch size: 1 (si Ã©s aplicable)
```

#### B. Variables d'entorn RunPod:

```bash
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

AixÃ² fa que PyTorch fragmenti menys la memÃ²ria, evitant OOM per fragmentaciÃ³.

### 2. **Enable CPU offloading** (Si el node ho suporta)

Al workflow, busca opcions com:

- `vram_mode: lowvram`
- `cpu_offload: true`
- `model_management: cpu`

### 3. **Split processing** (AvanÃ§at)

Si el model ho suporta:

- Genera frames en batches petits
- Usa nodes de chunking/tiling
- Pipeline multi-etapa

### 4. **Debug mode** (Per confirmar OOM)

Afegeix a RunPod:

```bash
CUDA_LAUNCH_BLOCKING=1
```

AixÃ² farÃ  que CUDA doni errors mÃ©s verbosos (perÃ² serÃ  mÃ©s lent).

---

## ğŸ§ª Testing Recomanat

### Pas 1: Confirmar OOM

```bash
# SSH al pod abans de cÃ³rrer workflow
watch -n 1 nvidia-smi

# VeurÃ s la VRAM pujar fins 100% just abans del crash
```

### Pas 2: Test amb cÃ rrega reduÃ¯da

Prova workflow amb:

- 32 frames @ 512x512 (cÃ rrega ~10GB)
- Si funciona â†’ confirmat OOM
- Si falla â†’ altre problema

### Pas 3: Incremental testing

Augmenta gradualment:

1. 32 frames â†’ OK?
2. 60 frames â†’ OK?
3. 121 frames â†’ Crash (lÃ­mit trobat)

---

## ğŸ“ Millores Implementades

### Al `comfy-entrypoint.sh`:

```bash
# Afegit trap per errors
trap 'echo "[ERROR] ComfyUI terminated (exit $?)"' ERR

# Afegit GPU memory logging
nvidia-smi --query-gpu=memory.used,memory.free,memory.total

# Afegit CUDA config
export PYTORCH_CUDA_ALLOC_CONF="${PYTORCH_CUDA_ALLOC_CONF:-max_split_size_mb:512}"
```

Ara els logs mostraran:

- GPU memory abans d'arrencar
- Exit code si ComfyUI crasheja
- Millor fragmentaciÃ³ de memÃ²ria

---

## ğŸ”„ Alternatives

### OpciÃ³ A: GPU mÃ©s gran

RunPod amb RTX A6000 (48GB) o A100 (80GB) permetria cÃ³rrer el workflow complet.

### OpciÃ³ B: Model mÃ©s petit

Usa variants del model:

- WanVideo 7B (en comptes de 14B)
- FP16 â†’ FP8 o INT8 quantitzat
- Distilled/pruned versions

### OpciÃ³ C: Resolution scaling

Genera a baixa resoluciÃ³, desprÃ©s upscale amb un model separat:

- Generate: 480x360 @ 121 frames
- Upscale: VideoUpscale node per frame

---

## ğŸ“Š Memory Budget Estimat

| Config         | Model | Activations | Total | RTX 4090 | Status   |
| -------------- | ----- | ----------- | ----- | -------- | -------- |
| 704x704 Ã— 121f | 14GB  | 18GB        | 32GB  | 22GB     | âŒ OOM   |
| 512x512 Ã— 121f | 14GB  | 10GB        | 24GB  | 22GB     | âš ï¸ Tight |
| 512x512 Ã— 60f  | 14GB  | 5GB         | 19GB  | 22GB     | âœ… OK    |
| 480x360 Ã— 121f | 14GB  | 6GB         | 20GB  | 22GB     | âœ… OK    |

---

## ğŸš€ Action Items

1. âœ… **Rebuild imatges amb millor logging** (fet)
2. â³ **Test workflow amb 60 frames @ 512x512**
3. â³ **Afegir variables PYTORCH_CUDA_ALLOC_CONF a template**
4. â³ **Documentar memory limits per model**
5. â³ **Considerar auto-scaling de resoluciÃ³ segons VRAM**

---

## ğŸ“– ReferÃ¨ncies

- [PyTorch CUDA Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)
- [CUDA Out of Memory Best Practices](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory)
- [ComfyUI Memory Optimization](https://github.com/comfyanonymous/ComfyUI/wiki/Optimizations)

---

**ConclusiÃ³:** El workflow es para per **Out of Memory**. Redueix resoluciÃ³ o frames per a RTX 4090. Amb les millores de logging, ara veurÃ s l'estat de memÃ²ria abans del crash.
